{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leahmdmartins10/Phishing-Email-Detection-System-BERT/blob/main/ver_3_June_28_Phishing_Email_Detection_System_Using_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATUNAnXVRrST"
      },
      "source": [
        "# ðŸ“§ Phishing Email Detection System Using BERT\n",
        "\n",
        "In this project, we aim to build a phishing email detection model using deep learning techniques, with a focus on the BERT (Bidirectional Encoder Representations from Transformers) architecture.\n",
        "\n",
        "Phishing emails are deceptive messages designed to trick users into revealing sensitive information. As attackers increasingly use AI to craft convincing emails, traditional rule-based filters fall short. This motivates the need for a more intelligent, language-aware detection system.\n",
        "\n",
        "We begin by loading and preprocessing real-world phishing and legitimate email datasets. After tokenizing the data, we will train and evaluate a fine-tuned BERT model, and compare its performance to a logistic regression baseline. Our objective is to build a model that accurately classifies emails as \"phishing\" or \"safe\" using language patterns and contextual understanding.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OEO-zjwEBxN"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "#KaggkeAPIKey = userdata.get('KaggleAPIKey')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95AQtTcrR9Jj"
      },
      "source": [
        "---\n",
        "# Mounting the google drive\n",
        "We have to mount the google drive seeing as the files for the datasets are stored there"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ELdb6e_bITxX",
        "outputId": "ea57f801-f361-4067-99ca-4942453d94e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FEMWGefsz9T"
      },
      "source": [
        "---\n",
        "# Cleaning Data across Datasets\n",
        "Making all data sets consistent in labeling, data type and format:\n",
        "\n",
        "1. \"body\": Holds the body of all emails.\n",
        "2. \"urls\": Holds the boolean value for if a url is present or not (1: url, 0: no url)\n",
        "3. \"label\": Holds the boolen value for if an email is Phishing or Safe (1: phishing, 0 not phishing)\n",
        "\n",
        "\n",
        "- REMOVING UNPARSABLE/ ILLEGAL DATA\n",
        "- You can view all data at \"APS360_Final_Cleaned_Data\" in shared folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "djpJFjk_3HY0",
        "outputId": "72b49f46-4ca8-4502-8181-c624410bf92d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xlsxwriter\n",
            "  Downloading xlsxwriter-3.2.5-py3-none-any.whl.metadata (2.7 kB)\n",
            "Downloading xlsxwriter-3.2.5-py3-none-any.whl (172 kB)\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/172.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m172.3/172.3 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.5\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install xlsxwriter\n",
        "!pip install pandas openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKAYArW12hB3",
        "outputId": "cf2e5eaf-900a-481e-8e26-21429a002a6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done! Cleaned Excel file saved at:\n",
            "/content/drive/MyDrive/APS360 Notes/Datasets/APS360_Final_Cleaned_Data.xlsx\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "#Folder with your CSVs\n",
        "source_folder = '/content/drive/MyDrive/APS360 Notes/Datasets'\n",
        "output_excel_path = os.path.join(source_folder, 'APS360_Final_Cleaned_Data.xlsx')\n",
        "\n",
        "#Patterns to detect illegal Excel characters and ANSI sequences\n",
        "ansi_pattern = re.compile(r'[\\x1B\\x1b]\\[[0-9;]*[A-Za-z]|[0-9]+;[0-9]+[Hf]')\n",
        "illegal_excel_chars = re.compile(r\"[\\x00-\\x08\\x0B-\\x1F]\")\n",
        "\n",
        "#Function to check if a row contains illegal characters\n",
        "def row_has_illegal_data(row):\n",
        "    return any(\n",
        "        ansi_pattern.search(str(cell)) or illegal_excel_chars.search(str(cell))\n",
        "        for cell in row\n",
        "    )\n",
        "\n",
        "#Create ExcelWriter object\n",
        "with pd.ExcelWriter(output_excel_path, engine='openpyxl') as writer:\n",
        "    for filename in os.listdir(source_folder):\n",
        "        if filename.endswith('.csv'):\n",
        "            filepath = os.path.join(source_folder, filename)\n",
        "\n",
        "            try:\n",
        "                df = pd.read_csv(filepath, on_bad_lines='skip', encoding='utf-8', engine='python')\n",
        "            except Exception as e:\n",
        "                print(f\"Skipping {filename} due to read error: {e}\")\n",
        "                continue\n",
        "\n",
        "            #Drop rows with illegal characters\n",
        "            df = df[~df.apply(row_has_illegal_data, axis=1)]\n",
        "\n",
        "            #Clean and rename columns\n",
        "            df.columns = [col.strip() for col in df.columns]\n",
        "            col_map = {}\n",
        "            for col in df.columns:\n",
        "                if col.lower() in ['email text', 'text']:\n",
        "                    col_map[col] = 'body'\n",
        "                elif col.lower() == 'email type':\n",
        "                    col_map[col] = 'label'\n",
        "            df = df.rename(columns=col_map)\n",
        "\n",
        "            #Add 'urls' column if missing\n",
        "            if 'urls' not in df.columns and 'body' in df.columns:\n",
        "                df['urls'] = df['body'].astype(str).apply(lambda x: 1 if 'http' in x else 0)\n",
        "\n",
        "            #Keep only ['body', 'urls', 'label']\n",
        "            keep_cols = [col for col in ['body', 'urls', 'label'] if col in df.columns]\n",
        "            df = df[keep_cols]\n",
        "\n",
        "            #Write sheet to Excel\n",
        "            sheet_name = os.path.splitext(filename)[0][:31]\n",
        "            try:\n",
        "                df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to write sheet for {filename}: {e}\")\n",
        "\n",
        "print(f\"Done! Cleaned Excel file saved at:\\n{output_excel_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpSuinQe1vKb"
      },
      "source": [
        "---\n",
        "#Combine Data into One Large Dataset\n",
        "\n",
        "- Takes all csv files and merges into one giant data set.\n",
        "- Removes empty and null rows.\n",
        "- Randomly shuffles and rearranges data.\n",
        "- Makes sure that \"label\" and \"urls\" data is numerical later processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTgcBQzS-Cpb"
      },
      "outputs": [],
      "source": [
        "#This is a function Force string/int labels to integer 0 or 1\n",
        "#Will be used later in combination (for cleaning purposes)\n",
        "\n",
        "def clean_numerics(x):\n",
        "    x_str = str(x).strip().lower()\n",
        "    if x_str in ['1', 'phishing email']:\n",
        "        return 1\n",
        "    elif x_str in ['0', 'safe email']:\n",
        "        return 0\n",
        "    else:\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6EwNNSm11UB"
      },
      "outputs": [],
      "source": [
        "#Load all sheets\n",
        "all_sheets = pd.read_excel(output_excel_path, sheet_name=None)\n",
        "\n",
        "#Concatenate all sheets into one DataFrame\n",
        "phishing_df = pd.concat(all_sheets.values(), ignore_index=True)\n",
        "\n",
        "#Drop rows with missing values (if any)\n",
        "phishing_df = phishing_df.dropna()\n",
        "\n",
        "#Shuffle dataset\n",
        "phishing_df = phishing_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "#Checks that this data is numerical\n",
        "phishing_df['label'] = phishing_df['label'].apply(clean_numerics)\n",
        "phishing_df['urls'] = phishing_df['urls'].apply(clean_numerics)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIjBoOuuDN5t"
      },
      "source": [
        "---\n",
        "#Split Tensor Data into Training Validation and Testing Datasets\n",
        "\n",
        "- Randomly split the encoded email data into 70% training, 15% validation, and 15% test sets.\n",
        "- Each split contains input tensors from the tokenization (input_ids, attention_mask) along with corresponding labels and URL indicators (from phishing_df ).\n",
        "- This prepares the data for use in training and evaluating an AI classification model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SAKxCtwDZ5A"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "\n",
        "#Convert labels and urls to tensors\n",
        "label = torch.tensor(phishing_df['label'].values)\n",
        "urls = torch.tensor(phishing_df['urls'].values)\n",
        "\n",
        "#First split into training data for 70% and temp data (vaidation + testing) for 30%\n",
        "train_idx, temp_idx = train_test_split(range(len(label)), test_size=0.3, random_state=42)\n",
        "\n",
        "#Then split temp into validation and testing 15% each\n",
        "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_NseqZUK7_u"
      },
      "outputs": [],
      "source": [
        "# splitting the bodies for train, val, and test data\n",
        "train_bodies = phishing_df['body'][train_idx].tolist()\n",
        "val_bodies = phishing_df['body'][val_idx].tolist()\n",
        "test_bodies = phishing_df['body'][test_idx].tolist()\n",
        "\n",
        "# converting the training, val, and test urls and labels to tensors\n",
        "train_urls = torch.tensor(phishing_df['urls'][train_idx].tolist())\n",
        "train_labels = torch.tensor(phishing_df['label'][train_idx].tolist())\n",
        "\n",
        "\n",
        "val_urls = torch.tensor(phishing_df['urls'][val_idx].tolist())\n",
        "val_labels = torch.tensor(phishing_df['label'][val_idx].tolist())\n",
        "\n",
        "\n",
        "test_urls = torch.tensor(phishing_df['urls'][test_idx].tolist())\n",
        "test_labels = torch.tensor(phishing_df['label'][test_idx].tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDcf_h7TNIMz"
      },
      "source": [
        "# **Tokenize the training, validation, and testing bodies**\n",
        "We are now tokenizing the data that we have previously split. This tokenizing code has been repurposed from Asmita's code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272,
          "referenced_widgets": [
            "b76924a436844fc2ab8c59ab8309a01b",
            "37eafc8306074cfebd68a77bc7a8129d",
            "d2f46d3dd8c54c9882ec377a8b1aba21",
            "fae045abfdc9483fb577cc2ef756e24e",
            "95dc1090dbfc4a8994c044d0b9f2dd71",
            "3c2d4674f6f94f2ea89e1d2fa0f8bb36",
            "c57159c615a148c3aba326e579bacac3",
            "2d2222c35294464b92f648b898ada3c6",
            "e91092e7fc3843f4950a5105215b82ba",
            "252954414eb94fb58a488c7bba2bf08a",
            "5832341fdf1d424e876933220a16da0e",
            "e268b50b8ccc42c08d47624f3b6f71c3",
            "81cb7e0a2dc84757b8e7e410ad98865e",
            "1b186b7dce854c3da6cb3809315fac4c",
            "ef4e48221000497b8a184968a400ed00",
            "39c041675ea24950bd1bd4d496ba2ebf",
            "a1466c0ba474421b88183c6392ac088e",
            "ba009f0095884ae5a9b3922075d6b86d",
            "aa296ce2b6d14144adac4af807bda0a6",
            "a1988ef6da4840659eec319246de98c8",
            "0268838f2cb04337b6ec842e16f6793a",
            "cb4f2c7cbd0548c09d2a7da86b8e4521",
            "d90f10b4dfca422cb8e3a15e8ced7b15",
            "1550a6cc3e65468aa537e225b6d1609f",
            "7439fcbfa2264f95acb9ab1af4ad3d4d",
            "ac38228f16a74ec98b32de7150480c43",
            "dff911121c9047d59bf5f7cb5555781c",
            "a364416efa9d482b8b8cc3b1d67d29b3",
            "2bd956ddd1f44331adbfecfa7c884d47",
            "47e0be0b80bd4211ae33c37d9c2b1e78",
            "3e16c9e6c8d64f1bac25054f195a8161",
            "1aa8486363a6477f9a98fa8c5c3d2399",
            "f68b46a558b4472981dc99ec9926e638",
            "0228423ce5a54e8bbcf05fd71539ff32",
            "32bc2cf7ef124767b07367e278c668eb",
            "3cd9dc6397cf42e883dc3acfa3ec285f",
            "4d2c257df789485092f48b5c6267f55d",
            "e5db3f8db8a84d46a79535085afc5a52",
            "8e447b08dfa443de98461f07a76ee4b9",
            "80d955b55408464ab58771be412f41f7",
            "582264c3a6a54b9089930ce424cfc49a",
            "a9a8d034fee242f482fcadb5646594a6",
            "c367c2cd1de24f00a75895690321301c",
            "fa6a0991197742009bff9c8fb97e07df"
          ]
        },
        "id": "OXgS5VIeNOQn",
        "outputId": "22084723-462b-4371-9606-ddf70c7a99e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b76924a436844fc2ab8c59ab8309a01b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e268b50b8ccc42c08d47624f3b6f71c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d90f10b4dfca422cb8e3a15e8ced7b15"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0228423ce5a54e8bbcf05fd71539ff32"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import BertTokenizerFast\n",
        "import torch\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "#Tokenize email bodies\n",
        "tokenizedTraining = tokenizer(\n",
        "    train_bodies,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "tokenizedValidation = tokenizer(\n",
        "    val_bodies,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "tokenizedTest = tokenizer(\n",
        "    test_bodies,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXKGGjA0rGKb",
        "outputId": "8b88be76-65f6-42c4-9be6-1da1336015e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training input_ids shape: torch.Size([46988, 512])\n",
            "Training attention_mask shape: torch.Size([46988, 512])\n",
            "greetings from dubai, this letter must come to you as a big surprise, but i believe it is only a day that people meet and become great friends and business partners. i am mr. arif shaikh, currently chief credit & risk officer with a reputable bank here in u. a. e. i write you this proposal in good faith, believing that i can trust you with the information i am about to reveal to you. i have an urgent and very confidential business proposition for you. on november 6, 2000, an iraqi foreign oil consultant / contractor with the chevron petroleum corporation, mr mohammad al nasser made a ( fixed deposit ) for 36 calendar months, valued at us $ 17, 500, 000. 00 ( seventeen million five hundred thousand dollars only ) in my bank and i happen to be his account officer before i was moved to my present position recently. upon maturity in 2003, as his account officer and as well the bank manger, it is my duty to notify him on the maturity date so i sent a routine notification to his forwarding address but the letter was returned undelivered. after sometime, i tried sending back the letter, but it was again returned and finally i discovered from his contract employers, chevron petroleum corporation that mr. mohammad al nasser died as a result of torture in the hand of saddam hussein ( former iraqi president ) during one of his trips to his country iraq, as he was accused of leaking information to the americans. on further investigation, i discovered that mr. al nasser ' s family wife and two sons died during the gulf war in iraq and was the reason why he did not declare any next of kin or relation in all his official documents, including his bank deposit paperwork in my bank and did not leave any will. this sum of us $ 17, 500, 000. 00 have been floating and placed under dormant / unserviceable account by my bank management since no one have heard from the owner since 2003. i wish to let you know that all the investigation i have made so far, my bank management is not aware of it, i am the only one that have the information. with the recent change of government in my country and with their efforts to support the united nations in checkmating terrorism aid in the u. a. e. by end of this year, the government will pass a new financial control law which will give the government authority to interrogate account owners of above $ 5, 000, 000. 00 to explain the source of the funds\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1])\n"
          ]
        }
      ],
      "source": [
        "#print the attention mask and tokenized list for first email\n",
        "print(\"Training input_ids shape:\", tokenizedTraining['input_ids'].shape)\n",
        "print(\"Training attention_mask shape:\", tokenizedTraining['attention_mask'].shape)\n",
        "decoded_input_ids = tokenizer.decode(tokenizedTraining['input_ids'][0], skip_special_tokens=True)\n",
        "print(decoded_input_ids)\n",
        "print(tokenizedTraining['attention_mask'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh4UwW6cRhwd"
      },
      "source": [
        "# **Converting the Tokenized Data to Complete Tensors**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_J-up2MXNCcn",
        "outputId": "2452b376-0669-41a8-956f-21182b285dd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset size: 46988\n",
            "Validation dataset size: 10069\n",
            "Testing dataset size: 10069\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "completeTraining_dataset = TensorDataset(\n",
        "    tokenizedTraining['input_ids'],\n",
        "    tokenizedTraining['attention_mask'],\n",
        "    train_labels,\n",
        "    train_urls\n",
        ")\n",
        "\n",
        "completeValidation_dataset = TensorDataset(\n",
        "    tokenizedValidation['input_ids'],\n",
        "    tokenizedValidation['attention_mask'],\n",
        "    val_labels,\n",
        "    val_urls\n",
        ")\n",
        "\n",
        "completeTesting_dataset = TensorDataset(\n",
        "    tokenizedTest['input_ids'],\n",
        "    tokenizedTest['attention_mask'],\n",
        "    test_labels,\n",
        "    test_urls\n",
        ")\n",
        "\n",
        "print(\"Training dataset size:\", len(completeTraining_dataset))\n",
        "print(\"Validation dataset size:\", len(completeValidation_dataset))\n",
        "print(\"Testing dataset size:\", len(completeTesting_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLDr7tB58DH9"
      },
      "source": [
        "# Tokenization for Logisitic Regression\n",
        "\n",
        "This implementation of tokenization will be used specifically for the logistic regression model. The implementation steps include:\n",
        "  1. Vectorizing the sentences\n",
        "  2. Counting the occurances of words\n",
        "  3. Vectorizing the numbers for the corresponding words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y60AkIXn9pKK"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,3))\n",
        "\n",
        "train_bow = vectorizer.fit_transform(train_bodies)\n",
        "val_bow = vectorizer.transform(val_bodies)\n",
        "test_bow = vectorizer.transform(test_bodies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6y67GCtsBusT"
      },
      "source": [
        "# Cleaning up RAM usage **(Place this block at the bottom of the code for now please)**\n",
        "We are going to clean up some data to prevent high usage of memory. Please note, you can only run this cell once if you don't reinstantiate these variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrtdhicOB4oJ",
        "outputId": "7c71037f-4724-48f9-8ed7-e7d415a1600d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "import gc\n",
        "\n",
        "del phishing_df\n",
        "\n",
        "del train_bodies, val_bodies, test_bodies\n",
        "\n",
        "del tokenizedTraining, tokenizedValidation, tokenizedTest\n",
        "\n",
        "gc.collect()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}