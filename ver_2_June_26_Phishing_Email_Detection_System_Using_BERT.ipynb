{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leahmdmartins10/Phishing-Email-Detection-System-BERT/blob/main/ver_2_June_26_Phishing_Email_Detection_System_Using_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“§ Phishing Email Detection System Using BERT\n",
        "\n",
        "In this project, we aim to build a phishing email detection model using deep learning techniques, with a focus on the BERT (Bidirectional Encoder Representations from Transformers) architecture.\n",
        "\n",
        "Phishing emails are deceptive messages designed to trick users into revealing sensitive information. As attackers increasingly use AI to craft convincing emails, traditional rule-based filters fall short. This motivates the need for a more intelligent, language-aware detection system.\n",
        "\n",
        "We begin by loading and preprocessing real-world phishing and legitimate email datasets. After tokenizing the data, we will train and evaluate a fine-tuned BERT model, and compare its performance to a logistic regression baseline. Our objective is to build a model that accurately classifies emails as \"phishing\" or \"safe\" using language patterns and contextual understanding.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ATUNAnXVRrST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "#KaggkeAPIKey = userdata.get('KaggleAPIKey')"
      ],
      "metadata": {
        "id": "7OEO-zjwEBxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Mounting the google drive\n",
        "We have to mount the google drive seeing as the files for the datasets are stored there"
      ],
      "metadata": {
        "id": "95AQtTcrR9Jj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELdb6e_bITxX",
        "outputId": "27a81307-5096-4de9-bdce-effa38e97c5c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Cleaning Data across Datasets\n",
        "Making all data sets consistent in labeling, data type and format:\n",
        "\n",
        "1. \"body\": Holds the body of all emails.\n",
        "2. \"urls\": Holds the boolean value for if a url is present or not (1: url, 0: no url)\n",
        "3. \"label\": Holds the boolen value for if an email is Phishing or Safe (1: phishing, 0 not phishing)\n",
        "\n",
        "\n",
        "- REMOVING UNPARSABLE/ ILLEGAL DATA\n",
        "- You can view all data at \"APS360_Final_Cleaned_Data\" in shared folder"
      ],
      "metadata": {
        "id": "-FEMWGefsz9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xlsxwriter\n",
        "!pip install pandas openpyxl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djpJFjk_3HY0",
        "outputId": "b201bcc8-2a4a-43e7-ac74-baf60f402c08",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xlsxwriter in /usr/local/lib/python3.11/dist-packages (3.2.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "#Folder with your CSVs\n",
        "source_folder = '/content/drive/MyDrive/APS360 Notes/Datasets'\n",
        "output_excel_path = os.path.join(source_folder, 'APS360_Final_Cleaned_Data.xlsx')\n",
        "\n",
        "#Patterns to detect illegal Excel characters and ANSI sequences\n",
        "ansi_pattern = re.compile(r'[\\x1B\\x1b]\\[[0-9;]*[A-Za-z]|[0-9]+;[0-9]+[Hf]')\n",
        "illegal_excel_chars = re.compile(r\"[\\x00-\\x08\\x0B-\\x1F]\")\n",
        "\n",
        "#Function to check if a row contains illegal characters\n",
        "def row_has_illegal_data(row):\n",
        "    return any(\n",
        "        ansi_pattern.search(str(cell)) or illegal_excel_chars.search(str(cell))\n",
        "        for cell in row\n",
        "    )\n",
        "\n",
        "#Create ExcelWriter object\n",
        "with pd.ExcelWriter(output_excel_path, engine='openpyxl') as writer:\n",
        "    for filename in os.listdir(source_folder):\n",
        "        if filename.endswith('.csv'):\n",
        "            filepath = os.path.join(source_folder, filename)\n",
        "\n",
        "            try:\n",
        "                df = pd.read_csv(filepath, on_bad_lines='skip', encoding='utf-8', engine='python')\n",
        "            except Exception as e:\n",
        "                print(f\"Skipping {filename} due to read error: {e}\")\n",
        "                continue\n",
        "\n",
        "            #Drop rows with illegal characters\n",
        "            df = df[~df.apply(row_has_illegal_data, axis=1)]\n",
        "\n",
        "            #Clean and rename columns\n",
        "            df.columns = [col.strip() for col in df.columns]\n",
        "            col_map = {}\n",
        "            for col in df.columns:\n",
        "                if col.lower() in ['email text', 'text']:\n",
        "                    col_map[col] = 'body'\n",
        "                elif col.lower() == 'email type':\n",
        "                    col_map[col] = 'label'\n",
        "            df = df.rename(columns=col_map)\n",
        "\n",
        "            #Add 'urls' column if missing\n",
        "            if 'urls' not in df.columns and 'body' in df.columns:\n",
        "                df['urls'] = df['body'].astype(str).apply(lambda x: 1 if 'http' in x else 0)\n",
        "\n",
        "            #Keep only ['body', 'urls', 'label']\n",
        "            keep_cols = [col for col in ['body', 'urls', 'label'] if col in df.columns]\n",
        "            df = df[keep_cols]\n",
        "\n",
        "            #Write sheet to Excel\n",
        "            sheet_name = os.path.splitext(filename)[0][:31]\n",
        "            try:\n",
        "                df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to write sheet for {filename}: {e}\")\n",
        "\n",
        "print(f\"Done! Cleaned Excel file saved at:\\n{output_excel_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKAYArW12hB3",
        "outputId": "4a633a81-91dd-49c8-eb0f-077cfc0f4549"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done! Cleaned Excel file saved at:\n",
            "/content/drive/MyDrive/APS360 Notes/Datasets/APS360_Final_Cleaned_Data.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#Combine Data into One Large Dataset\n",
        "\n",
        "- Takes all csv files and merges into one giant data set.\n",
        "- Removes empty and null rows.\n",
        "- Randomly shuffles and rearranges data.\n",
        "- Makes sure that \"label\" and \"urls\" data is numerical later processing"
      ],
      "metadata": {
        "id": "IpSuinQe1vKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This is a function Force string/int labels to integer 0 or 1\n",
        "#Will be used later in combination (for cleaning purposes)\n",
        "\n",
        "def clean_numerics(x):\n",
        "    x_str = str(x).strip().lower()\n",
        "    if x_str in ['1', 'phishing email']:\n",
        "        return 1\n",
        "    elif x_str in ['0', 'safe email']:\n",
        "        return 0\n",
        "    else:\n",
        "        return 0"
      ],
      "metadata": {
        "id": "MTgcBQzS-Cpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load all sheets\n",
        "all_sheets = pd.read_excel(output_excel_path, sheet_name=None)\n",
        "\n",
        "#Concatenate all sheets into one DataFrame\n",
        "phishing_df = pd.concat(all_sheets.values(), ignore_index=True)\n",
        "\n",
        "#Drop rows with missing values (if any)\n",
        "phishing_df = phishing_df.dropna()\n",
        "\n",
        "#Shuffle dataset\n",
        "phishing_df = phishing_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "#Checks that this data is numerical\n",
        "phishing_df['label'] = phishing_df['label'].apply(clean_numerics)\n",
        "phishing_df['urls'] = phishing_df['urls'].apply(clean_numerics)\n"
      ],
      "metadata": {
        "id": "y6EwNNSm11UB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#Split Tensor Data into Training Validation and Testing Datasets\n",
        "\n",
        "- Randomly split the encoded email data into 70% training, 15% validation, and 15% test sets.\n",
        "- Each split contains input tensors from the tokenization (input_ids, attention_mask) along with corresponding labels and URL indicators (from phishing_df ).\n",
        "- This prepares the data for use in training and evaluating an AI classification model.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MIjBoOuuDN5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "\n",
        "#Convert labels and urls to tensors\n",
        "label = torch.tensor(phishing_df['label'].values)\n",
        "urls = torch.tensor(phishing_df['urls'].values)\n",
        "\n",
        "#First split into training data for 70% and temp data (vaidation + testing) for 30%\n",
        "train_idx, temp_idx = train_test_split(range(len(label)), test_size=0.3, random_state=42)\n",
        "\n",
        "#Then split temp into validation and testing 15% each\n",
        "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)\n"
      ],
      "metadata": {
        "id": "8SAKxCtwDZ5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting the bodies for train, val, and test data\n",
        "train_bodies = phishing_df['body'][train_idx].tolist()\n",
        "val_bodies = phishing_df['body'][val_idx].tolist()\n",
        "test_bodies = phishing_df['body'][test_idx].tolist()\n",
        "\n",
        "# converting the training, val, and test urls and labels to tensors\n",
        "train_urls = torch.tensor(phishing_df['urls'][train_idx].tolist())\n",
        "train_labels = torch.tensor(phishing_df['label'][train_idx].tolist())\n",
        "\n",
        "\n",
        "val_urls = torch.tensor(phishing_df['urls'][val_idx].tolist())\n",
        "val_labels = torch.tensor(phishing_df['label'][val_idx].tolist())\n",
        "\n",
        "\n",
        "test_urls = torch.tensor(phishing_df['urls'][test_idx].tolist())\n",
        "test_labels = torch.tensor(phishing_df['label'][test_idx].tolist())"
      ],
      "metadata": {
        "id": "X_NseqZUK7_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenize the training, validation, and testing bodies**\n",
        "We are now tokenizing the data that we have previously split. This tokenizing code has been repurposed from Asmita's code."
      ],
      "metadata": {
        "id": "fDcf_h7TNIMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizerFast\n",
        "import torch\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "#Tokenize email bodies\n",
        "tokenizedTraining = tokenizer(\n",
        "    train_bodies,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "tokenizedValidation = tokenizer(\n",
        "    val_bodies,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "tokenizedTest = tokenizer(\n",
        "    test_bodies,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OXgS5VIeNOQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print the attention mask and tokenized list for first email\n",
        "print(\"Training input_ids shape:\", tokenizedTraining['input_ids'].shape)\n",
        "print(\"Training attention_mask shape:\", tokenizedTraining['attention_mask'].shape)\n",
        "decoded_input_ids = tokenizer.decode(tokenizedTraining['input_ids'][0], skip_special_tokens=True)\n",
        "print(decoded_input_ids)\n",
        "print(tokenizedTraining['attention_mask'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXKGGjA0rGKb",
        "outputId": "7983474a-5e86-41f8-f912-f90ca71f00bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training input_ids shape: torch.Size([46988, 512])\n",
            "Training attention_mask shape: torch.Size([46988, 512])\n",
            "greetings from dubai, this letter must come to you as a big surprise, but i believe it is only a day that people meet and become great friends and business partners. i am mr. arif shaikh, currently chief credit & risk officer with a reputable bank here in u. a. e. i write you this proposal in good faith, believing that i can trust you with the information i am about to reveal to you. i have an urgent and very confidential business proposition for you. on november 6, 2000, an iraqi foreign oil consultant / contractor with the chevron petroleum corporation, mr mohammad al nasser made a ( fixed deposit ) for 36 calendar months, valued at us $ 17, 500, 000. 00 ( seventeen million five hundred thousand dollars only ) in my bank and i happen to be his account officer before i was moved to my present position recently. upon maturity in 2003, as his account officer and as well the bank manger, it is my duty to notify him on the maturity date so i sent a routine notification to his forwarding address but the letter was returned undelivered. after sometime, i tried sending back the letter, but it was again returned and finally i discovered from his contract employers, chevron petroleum corporation that mr. mohammad al nasser died as a result of torture in the hand of saddam hussein ( former iraqi president ) during one of his trips to his country iraq, as he was accused of leaking information to the americans. on further investigation, i discovered that mr. al nasser ' s family wife and two sons died during the gulf war in iraq and was the reason why he did not declare any next of kin or relation in all his official documents, including his bank deposit paperwork in my bank and did not leave any will. this sum of us $ 17, 500, 000. 00 have been floating and placed under dormant / unserviceable account by my bank management since no one have heard from the owner since 2003. i wish to let you know that all the investigation i have made so far, my bank management is not aware of it, i am the only one that have the information. with the recent change of government in my country and with their efforts to support the united nations in checkmating terrorism aid in the u. a. e. by end of this year, the government will pass a new financial control law which will give the government authority to interrogate account owners of above $ 5, 000, 000. 00 to explain the source of the funds\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Converting the Tokenized Data to Complete Tensors**\n"
      ],
      "metadata": {
        "id": "Eh4UwW6cRhwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "completeTraining_dataset = TensorDataset(\n",
        "    tokenizedTraining['input_ids'],\n",
        "    tokenizedTraining['attention_mask'],\n",
        "    train_labels,\n",
        "    train_urls\n",
        ")\n",
        "\n",
        "completeValidation_dataset = TensorDataset(\n",
        "    tokenizedValidation['input_ids'],\n",
        "    tokenizedValidation['attention_mask'],\n",
        "    val_labels,\n",
        "    val_urls\n",
        ")\n",
        "\n",
        "completeTesting_dataset = TensorDataset(\n",
        "    tokenizedTest['input_ids'],\n",
        "    tokenizedTest['attention_mask'],\n",
        "    test_labels,\n",
        "    test_urls\n",
        ")\n",
        "\n",
        "print(\"Training dataset size:\", len(completeTraining_dataset))\n",
        "print(\"Validation dataset size:\", len(completeValidation_dataset))\n",
        "print(\"Testing dataset size:\", len(completeTesting_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_J-up2MXNCcn",
        "outputId": "f08f2630-e5d5-4160-e537-94f00be4b89e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset size: 46988\n",
            "Validation dataset size: 10069\n",
            "Testing dataset size: 10069\n"
          ]
        }
      ]
    }
  ]
}