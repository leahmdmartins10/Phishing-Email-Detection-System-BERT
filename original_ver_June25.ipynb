{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leahmdmartins10/Phishing-Email-Detection-System-BERT/blob/main/Phishing_Email_Detection_System_Using_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“§ Phishing Email Detection System Using BERT\n",
        "\n",
        "In this project, we aim to build a phishing email detection model using deep learning techniques, with a focus on the BERT (Bidirectional Encoder Representations from Transformers) architecture.\n",
        "\n",
        "Phishing emails are deceptive messages designed to trick users into revealing sensitive information. As attackers increasingly use AI to craft convincing emails, traditional rule-based filters fall short. This motivates the need for a more intelligent, language-aware detection system.\n",
        "\n",
        "We begin by loading and preprocessing real-world phishing and legitimate email datasets. After tokenizing the data, we will train and evaluate a fine-tuned BERT model, and compare its performance to a logistic regression baseline. Our objective is to build a model that accurately classifies emails as \"phishing\" or \"safe\" using language patterns and contextual understanding.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ATUNAnXVRrST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "#KaggkeAPIKey = userdata.get('KaggleAPIKey')"
      ],
      "metadata": {
        "id": "7OEO-zjwEBxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Mounting the google drive\n",
        "We have to mount the google drive seeing as the files for the datasets are stored there"
      ],
      "metadata": {
        "id": "95AQtTcrR9Jj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELdb6e_bITxX",
        "outputId": "5dbe10e4-1701-4d5d-cfce-15b549e1a7fd",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Cleaning Data across Datasets\n",
        "Making all data sets consistent in labeling, data type and format:\n",
        "\n",
        "1. \"body\": Holds the body of all emails.\n",
        "2. \"urls\": Holds the boolean value for if a url is present or not (1: url, 0: no url)\n",
        "3. \"label\": Holds the boolen value for if an email is Phishing or Safe (1: phishing, 0 not phishing)\n",
        "\n",
        "\n",
        "- REMOVING UNPARSABLE/ ILLEGAL DATA\n",
        "- You can view all data at \"APS360_Final_Cleaned_Data\" in shared folder"
      ],
      "metadata": {
        "id": "-FEMWGefsz9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xlsxwriter\n",
        "!pip install pandas openpyxl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djpJFjk_3HY0",
        "outputId": "801351fd-2b91-467f-94ea-2f46d9c3306b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xlsxwriter in /usr/local/lib/python3.11/dist-packages (3.2.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "#Folder with your CSVs\n",
        "source_folder = '/content/drive/MyDrive/APS360 Notes/Datasets'\n",
        "output_excel_path = os.path.join(source_folder, 'APS360_Final_Cleaned_Data.xlsx')\n",
        "\n",
        "#Patterns to detect illegal Excel characters and ANSI sequences\n",
        "ansi_pattern = re.compile(r'[\\x1B\\x1b]\\[[0-9;]*[A-Za-z]|[0-9]+;[0-9]+[Hf]')\n",
        "illegal_excel_chars = re.compile(r\"[\\x00-\\x08\\x0B-\\x1F]\")\n",
        "\n",
        "#Function to check if a row contains illegal characters\n",
        "def row_has_illegal_data(row):\n",
        "    return any(\n",
        "        ansi_pattern.search(str(cell)) or illegal_excel_chars.search(str(cell))\n",
        "        for cell in row\n",
        "    )\n",
        "\n",
        "#Create ExcelWriter object\n",
        "with pd.ExcelWriter(output_excel_path, engine='openpyxl') as writer:\n",
        "    for filename in os.listdir(source_folder):\n",
        "        if filename.endswith('.csv'):\n",
        "            filepath = os.path.join(source_folder, filename)\n",
        "\n",
        "            try:\n",
        "                df = pd.read_csv(filepath, on_bad_lines='skip', encoding='utf-8', engine='python')\n",
        "            except Exception as e:\n",
        "                print(f\"Skipping {filename} due to read error: {e}\")\n",
        "                continue\n",
        "\n",
        "            #Drop rows with illegal characters\n",
        "            df = df[~df.apply(row_has_illegal_data, axis=1)]\n",
        "\n",
        "            #Clean and rename columns\n",
        "            df.columns = [col.strip() for col in df.columns]\n",
        "            col_map = {}\n",
        "            for col in df.columns:\n",
        "                if col.lower() in ['email text', 'text']:\n",
        "                    col_map[col] = 'body'\n",
        "                elif col.lower() == 'email type':\n",
        "                    col_map[col] = 'label'\n",
        "            df = df.rename(columns=col_map)\n",
        "\n",
        "            #Add 'urls' column if missing\n",
        "            if 'urls' not in df.columns and 'body' in df.columns:\n",
        "                df['urls'] = df['body'].astype(str).apply(lambda x: 1 if 'http' in x else 0)\n",
        "\n",
        "            #Keep only ['body', 'urls', 'label']\n",
        "            keep_cols = [col for col in ['body', 'urls', 'label'] if col in df.columns]\n",
        "            df = df[keep_cols]\n",
        "\n",
        "            #Write sheet to Excel\n",
        "            sheet_name = os.path.splitext(filename)[0][:31]\n",
        "            try:\n",
        "                df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to write sheet for {filename}: {e}\")\n",
        "\n",
        "print(f\"Done! Cleaned Excel file saved at:\\n{output_excel_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKAYArW12hB3",
        "outputId": "480108fc-ce14-4e74-a51b-14563dc8de80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done! Cleaned Excel file saved at:\n",
            "/content/drive/MyDrive/APS360 Notes/Datasets/APS360_Final_Cleaned_Data.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#Combine Data into One Large Dataset\n",
        "\n",
        "- Takes all csv files and merges into one giant data set.\n",
        "- Removes empty and null rows.\n",
        "- Randomly shuffles and rearranges data.\n",
        "- Makes sure that \"label\" and \"urls\" data is numerical later processing"
      ],
      "metadata": {
        "id": "IpSuinQe1vKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This is a function Force string/int labels to integer 0 or 1\n",
        "#Will be used later in combination (for cleaning purposes)\n",
        "\n",
        "def clean_numerics(x):\n",
        "    x_str = str(x).strip().lower()\n",
        "    if x_str in ['1', 'phishing email']:\n",
        "        return 1\n",
        "    elif x_str in ['0', 'safe email']:\n",
        "        return 0\n",
        "    else:\n",
        "        return 0"
      ],
      "metadata": {
        "id": "MTgcBQzS-Cpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load all sheets\n",
        "all_sheets = pd.read_excel(output_excel_path, sheet_name=None)\n",
        "\n",
        "#Concatenate all sheets into one DataFrame\n",
        "phishing_df = pd.concat(all_sheets.values(), ignore_index=True)\n",
        "\n",
        "#Drop rows with missing values (if any)\n",
        "phishing_df = phishing_df.dropna()\n",
        "\n",
        "#Shuffle dataset\n",
        "phishing_df = phishing_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "#Checks that this data is numerical\n",
        "phishing_df['label'] = phishing_df['label'].apply(clean_numerics)\n",
        "phishing_df['urls'] = phishing_df['urls'].apply(clean_numerics)\n"
      ],
      "metadata": {
        "id": "y6EwNNSm11UB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#Convert Text Data to Tensor Value\n",
        "\n",
        "Using a Hugging face Transformer: BertTokenizer that can take the text values and make it a tensor"
      ],
      "metadata": {
        "id": "t4SAlkr43UiU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "#Tokenize email bodies\n",
        "encoded = tokenizer(\n",
        "    phishing_df['body'].tolist(),\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "#Convert labels and urls to tensors\n",
        "label = torch.tensor(phishing_df['label'].values)\n",
        "urls = torch.tensor(phishing_df['urls'].values)\n"
      ],
      "metadata": {
        "id": "qzQIrEPz3sXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#Split Tensor Data into Training Validation and Testing Datasets\n",
        "\n",
        "- Randomly split the encoded email data into 70% training, 15% validation, and 15% test sets.\n",
        "- Each split contains input tensors from the tokenization (input_ids, attention_mask) along with corresponding labels and URL indicators (from phishing_df ).\n",
        "- This prepares the data for use in training and evaluating an AI classification model.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MIjBoOuuDN5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#First split into training data for 70% and temp data (vaidation + testing) for 30%\n",
        "train_idx, temp_idx = train_test_split(range(len(label)), test_size=0.3, random_state=42)\n",
        "\n",
        "#Then split temp into validation and testing 15% each\n",
        "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)\n"
      ],
      "metadata": {
        "id": "8SAKxCtwDZ5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Helper function to index data\n",
        "def select_tensors(indices):\n",
        "    input_ids = encoded['input_ids'][indices]\n",
        "    attention_mask = encoded['attention_mask'][indices]\n",
        "    label_heading = label[indices]\n",
        "    url_heading = urls[indices]\n",
        "    return input_ids, attention_mask, label, urls\n",
        "\n",
        "train_data = select_tensors(train_idx)\n",
        "val_data   = select_tensors(val_idx)\n",
        "test_data  = select_tensors(test_idx)\n"
      ],
      "metadata": {
        "id": "NzEkO1luDi8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check sizes\n",
        "print(\"Train size:\", train_data[0].shape[0])\n",
        "print(\"Val size:\", val_data[0].shape[0])\n",
        "print(\"Test size:\", test_data[0].shape[0])\n",
        "print(\"\\n\")\n",
        "\n",
        "#Check that total matches original\n",
        "total = train_data[0].shape[0] + val_data[0].shape[0] + test_data[0].shape[0]\n",
        "print(\"Total samples:\", total, \"| Original:\", len(label))\n",
        "print(\"\\n\")\n",
        "\n",
        "#Check tensor shapes for debugging\n",
        "print(\"Train input_ids shape:\", train_data[0].shape)\n",
        "print(\"Train attention_mask shape:\", train_data[1].shape)\n",
        "print(\"Train labels shape:\", train_data[2].shape)\n",
        "print(\"Train urls shape:\", train_data[3].shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iv1rzpJQFBZU",
        "outputId": "aaabb6be-1ee0-4eb7-be8f-2e9b4832709e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 46988\n",
            "Val size: 10069\n",
            "Test size: 10069\n",
            "\n",
            "\n",
            "Total samples: 67126 | Original: 67126\n",
            "\n",
            "\n",
            "Train input_ids shape: torch.Size([46988, 512])\n",
            "Train attention_mask shape: torch.Size([46988, 512])\n",
            "Train labels shape: torch.Size([67126])\n",
            "Train urls shape: torch.Size([67126])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# LEE PREVIOUS TIME: Splitting the Data Into Training Data, Validation Data, and Test Data"
      ],
      "metadata": {
        "id": "D5BRx-4ORoVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "trainSizedf = int(0.7 * len(df))\n",
        "valSizedf = int(0.15 * len(df))\n",
        "testSizedf = len(df) - trainSizedf - valSizedf\n",
        "\n",
        "dfTrain_Data, dfVal_Data, dfTest_Data = random_split(df, [trainSizedf, valSizedf, testSizedf])\n",
        "\n",
        "trainSizePhishing_Email = int(0.7 * len(Phishing_Email))\n",
        "valSizePhishing_Email = int(0.15 * len(Phishing_Email))\n",
        "testSizePhishing_Email = len(Phishing_Email) - trainSizePhishing_Email - valSizePhishing_Email\n",
        "\n",
        "Phishing_EmailTrain_Data, Phishing_EmailVal_Data, Phishing_EmailTest_Data = random_split(Phishing_Email, [trainSizePhishing_Email, valSizePhishing_Email, testSizePhishing_Email])"
      ],
      "metadata": {
        "id": "6QCTY2JLSk07"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
